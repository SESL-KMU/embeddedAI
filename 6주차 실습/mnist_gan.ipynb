{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUxSSuo8IXRF",
        "outputId": "90c48b49-7f44-4ee4-9b28-fb2cb950fe58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [001/100]  D Loss: 0.1141, G Loss: 3.1040\n",
            "Epoch [002/100]  D Loss: 0.1328, G Loss: 3.3868\n",
            "Epoch [003/100]  D Loss: 0.0940, G Loss: 4.0175\n",
            "Epoch [004/100]  D Loss: 0.1007, G Loss: 5.1259\n",
            "Epoch [005/100]  D Loss: 0.0797, G Loss: 3.4724\n",
            "Epoch [006/100]  D Loss: 0.1729, G Loss: 2.9989\n",
            "Epoch [007/100]  D Loss: 0.1354, G Loss: 3.2387\n",
            "Epoch [008/100]  D Loss: 0.1438, G Loss: 3.7516\n",
            "Epoch [009/100]  D Loss: 0.1320, G Loss: 3.8958\n",
            "Epoch [010/100]  D Loss: 0.3066, G Loss: 2.6786\n",
            "Epoch [011/100]  D Loss: 0.1824, G Loss: 3.5589\n",
            "Epoch [012/100]  D Loss: 0.2730, G Loss: 2.9390\n",
            "Epoch [013/100]  D Loss: 0.2475, G Loss: 3.0807\n",
            "Epoch [014/100]  D Loss: 0.2913, G Loss: 3.4162\n",
            "Epoch [015/100]  D Loss: 0.2575, G Loss: 3.1665\n",
            "Epoch [016/100]  D Loss: 0.2859, G Loss: 3.7177\n",
            "Epoch [017/100]  D Loss: 0.1628, G Loss: 4.1763\n",
            "Epoch [018/100]  D Loss: 0.2766, G Loss: 2.9310\n",
            "Epoch [019/100]  D Loss: 0.2658, G Loss: 3.8986\n",
            "Epoch [020/100]  D Loss: 0.2254, G Loss: 3.7210\n",
            "Epoch [021/100]  D Loss: 0.1731, G Loss: 4.2566\n",
            "Epoch [022/100]  D Loss: 0.1963, G Loss: 3.8526\n",
            "Epoch [023/100]  D Loss: 0.1727, G Loss: 4.6789\n",
            "Epoch [024/100]  D Loss: 0.1754, G Loss: 4.7743\n",
            "Epoch [025/100]  D Loss: 0.2792, G Loss: 3.4626\n",
            "Epoch [026/100]  D Loss: 0.2409, G Loss: 4.2908\n",
            "Epoch [027/100]  D Loss: 0.2857, G Loss: 3.4754\n",
            "Epoch [028/100]  D Loss: 0.2451, G Loss: 3.9425\n",
            "Epoch [029/100]  D Loss: 0.3665, G Loss: 3.9333\n",
            "Epoch [030/100]  D Loss: 0.3472, G Loss: 3.1865\n",
            "Epoch [031/100]  D Loss: 0.3240, G Loss: 3.8645\n",
            "Epoch [032/100]  D Loss: 0.3085, G Loss: 3.8170\n",
            "Epoch [033/100]  D Loss: 0.3278, G Loss: 3.8707\n",
            "Epoch [034/100]  D Loss: 0.3377, G Loss: 4.0427\n",
            "Epoch [035/100]  D Loss: 0.3534, G Loss: 3.5241\n",
            "Epoch [036/100]  D Loss: 0.2846, G Loss: 3.6259\n",
            "Epoch [037/100]  D Loss: 0.3141, G Loss: 3.4436\n",
            "Epoch [038/100]  D Loss: 0.2271, G Loss: 3.4769\n",
            "Epoch [039/100]  D Loss: 0.3268, G Loss: 3.4204\n",
            "Epoch [040/100]  D Loss: 0.3884, G Loss: 3.1707\n",
            "Epoch [041/100]  D Loss: 0.4221, G Loss: 3.2323\n",
            "Epoch [042/100]  D Loss: 0.4691, G Loss: 2.9820\n",
            "Epoch [043/100]  D Loss: 0.4651, G Loss: 3.0549\n",
            "Epoch [044/100]  D Loss: 0.4486, G Loss: 2.9701\n",
            "Epoch [045/100]  D Loss: 0.5499, G Loss: 2.8519\n",
            "Epoch [046/100]  D Loss: 0.5403, G Loss: 2.9092\n",
            "Epoch [047/100]  D Loss: 0.5377, G Loss: 3.0267\n",
            "Epoch [048/100]  D Loss: 0.6743, G Loss: 2.4442\n",
            "Epoch [049/100]  D Loss: 0.6566, G Loss: 2.7230\n",
            "Epoch [050/100]  D Loss: 0.5359, G Loss: 2.7197\n",
            "Epoch [051/100]  D Loss: 0.5085, G Loss: 2.7234\n",
            "Epoch [052/100]  D Loss: 0.4764, G Loss: 3.0488\n",
            "Epoch [053/100]  D Loss: 0.4682, G Loss: 2.4174\n",
            "Epoch [054/100]  D Loss: 0.6001, G Loss: 2.8180\n",
            "Epoch [055/100]  D Loss: 0.6509, G Loss: 2.6774\n"
          ]
        }
      ],
      "source": [
        "# PyTorch version of basic MNIST GAN (Colab-compatible)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.0002\n",
        "total_epoch = 100\n",
        "batch_size = 100\n",
        "n_hidden = 256\n",
        "n_input = 28 * 28\n",
        "n_noise = 128\n",
        "\n",
        "# MNIST dataset\n",
        "dataset = datasets.MNIST(root=\"./mnist/data/\", train=True, download=True,\n",
        "                         transform=transforms.ToTensor())\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Generator class\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(n_noise, n_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hidden, n_input),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "# Discriminator class\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(n_input, n_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hidden, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Models\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "\n",
        "# Sample folder\n",
        "os.makedirs(\"samples_ex\", exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(total_epoch):\n",
        "    for idx, (real_imgs, _) in enumerate(data_loader):\n",
        "        real_imgs = real_imgs.view(-1, n_input).to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Labels\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        z = torch.randn(batch_size, n_noise).to(device)\n",
        "        fake_imgs = generator(z)\n",
        "\n",
        "        real_output = discriminator(real_imgs)\n",
        "        fake_output = discriminator(fake_imgs.detach())\n",
        "\n",
        "        d_loss = criterion(real_output, real_labels) + criterion(fake_output, fake_labels)\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        z = torch.randn(batch_size, n_noise).to(device)\n",
        "        fake_imgs = generator(z)\n",
        "        output = discriminator(fake_imgs)\n",
        "        g_loss = criterion(output, real_labels)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1:03d}/{total_epoch}]  D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    # Save generated samples every 10 epochs\n",
        "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
        "        sample_z = torch.randn(10, n_noise).to(device)\n",
        "        samples = generator(sample_z).view(-1, 28, 28).cpu().data\n",
        "\n",
        "        fig, ax = plt.subplots(1, 10, figsize=(10, 1))\n",
        "        for i in range(10):\n",
        "            ax[i].imshow(samples[i], cmap='gray')\n",
        "            ax[i].axis('off')\n",
        "        plt.savefig(f\"samples_ex/{str(epoch).zfill(3)}.png\", bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n"
      ]
    }
  ]
}